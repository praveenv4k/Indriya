\documentclass{llncs}

%\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{graphicx,float,wrapfig}
\usepackage{epstopdf}
\usepackage{caption}
%\usepackage{subcaption}
\usepackage{subcaption}
\captionsetup{compatibility=false}
%
%\title{Experimental Platform for Human Robot Interaction based on Human Motions}
\title{An Intuitive Interface for designing Social behaviors based on Human motions}
%
\titlerunning{ExPhriMot}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Praveenkumar Vasudevan\inst{1} \and Gentiane Venture\inst{2}}
%
\authorrunning{Praveenkumar Vasudevan et al.} % abbreviated author list (for running head)
%\
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Praveenkumar Vasudevan, Gentiane Venture}
%
\institute{Graduate Student, \'{E}cole Centrale de Nantes, Nantes, France,\\
\email{praveenv4k@gmail.com}
\and
Associate Professor, Tokyo University of Agriculture and Technology, Japan\\
\email{venture@cc.tuat.ac.jp}}
\begin{document}

\maketitle   
\begin{abstract}
	Humans interacting with intelligent robots has been seen as a potential game changer of the future. In scenarios where robots coexist with humans in a social environment, understanding not only verbal communication, but also non-verbal communication is extremely inevitable. The non-verbal communication carries information such as intention, emotion and health of a human, that adds value to the way robots participate in an interaction. Additionally, the people who design interaction scenarios are from diverse fields who do not essentially have the required robot programming skills. In this paper we propose an easy to use and intuitive programming interface which gives the power to design robot behaviors taking into account human motions. We propose a distributed architecture which gives the capability to plug and play multi-modal motion recognition systems and diverse class of robots. We present results of Nao humanoid robot performing actions understanding human motions using kinect motion recognition system.
\keywords{human robot interaction, motion recognition, robot behaviors, kinect, nao}
\end{abstract}
%
\section{Introduction}
%
%Human-Robot interaction (HRI) has emerged as the most promising fields in the recent years owing to its immense potential in the fields of education, entertainment, elderly care and rehabilitation. 
The richness and diversity of Human Robot Interaction (HRI) has been described in \cite{Dautenhahn2007} as \emph{HRI is a challenging research field at the intersection of psychology, cognitive science, social sciences, artificial intelligence, computer science, robotics, engineering and human-computer interaction}. Goodrich\cite{Goodrich:2007:HIS:1348099.1348100} in his extensive survey proposed two main types of HRI namely Remote interaction or Teleoperation and Proximate interaction. The latter is particularly important where the humans and the robots are co-located (for example, service robots may be in the same room as humans). Proximate interaction has gained importance due to the successful encounters of putting robots to work with human beings. It has led to the development of a new class of robots called Social Robots. In his survey on social robotics, Yan et al. \cite{Yan2014} defines \emph{“A social robot is a robot which can execute designated tasks and the necessary condition turning a robot into a social robot is the ability to interact with humans by adhering to certain social cues and rules.”}

The Social robots already entered the human spaces as shown in Fig~\ref{fig:hri} as entertainers\cite{NaoTheRobot}, educators\cite{NaoTheRobot}, caring agents\cite{ASKNao} and personal assistants\cite{ProjectRomeo}\cite{PepperTheRobot}. The design and development of interaction systems need to be approached in a systematic manner wherein the robots should be able to understand the human motions and intentions in order to interact in a better way. To make it possible it is necessary to develop robotic systems with essential perceptive systems for efficient and natural interaction. Most often the on-board sensors on the robots fail to satisfy this demanding requirement due to various constraints like space, power and computational requirements. Hence consideration of augmenting exteroceptive sensors that are commonly available in the smart home/public environments is important.
\begin{figure}
\centering
\begin{subfigure}[t]{0.45\textwidth}
\includegraphics[width=\textwidth]{../thesis/assets/asknao.png}
\caption[Human Robot Interaction]{Autism Therapy$^{\cite{ASKNao}}$ }
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
\includegraphics[width=\textwidth]{../thesis/assets/nao_education.png}
\caption[Human Robot Interaction]{Education$^{\cite{NaoTheRobot}}$ }
\end{subfigure}

\begin{subfigure}[t]{0.45\textwidth}
\includegraphics[width=\textwidth]{../thesis/assets/romeo_interaction.png}
\caption[Human Robot Interaction]{Physical Interaction$^{\cite{ProjectRomeo}}$ }
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
\includegraphics[width=\textwidth]{../thesis/assets/pepper_interaction.png}
\caption[Human Robot Interaction]{Casual Interaction$^{\cite{PepperTheRobot}}$ }
\end{subfigure}
\caption[Human Robot Interaction]{Human Robot Interaction}
\label{fig:hri}
\end{figure}

%However when it comes to designing complex behaviors, traditional flow chart based approaches[2] increase the cognitive load on the end users.
Another important aspect in HRI is the fact that the users of such systems are from diverse backgrounds. People study various aspects such as robot ethics, social acceptance, liveliness, cultural influence etc., from various perspectives like Sociology, Psychology, Humanities and so on. So the tools needed to design behaviors of a social robot should be intuitive and user friendly.  With increased availability of social robots and cost effective motion recognition sensors, we could still observe a huge void which inhibits the exploitation of available technology for designing human motion driven robot behaviors.

The main contribution of this work will be to develop an application independent experimental platform wherein a social robot will be augmented with essential perceptual ability to understand human motions. The behavior design of such a social robot will be made possible by an easy to use behavior design interface. The resulting experimental platform could be used by people from interdisciplinary fields to design, evaluate and study specific aspects of the interaction between social robots and the human. We propose to use commercially available Kinect RGB-D cameras for the relative localization of human and robot and also as a gesture recognition system.
%
%The key problem statements of this work are
%\begin{itemize}
%\item Human Pose estimation and motion recognition
%\item Localization of the robot
%\item Behavior design interface
%\end{itemize}
%
\section{Related work}
%
%\subsection{Human Pose Detection}
%
	Vision based motion capture and analysis has been one of the first class citizens in the human motion understanding. It has been studied widely and a summary of all the approaches developed during the past three decades have been presented in the surveys\cite{Moeslund200690}\cite{Poppe20074}. Vision based human pose estimation has traditionally suffered from the requirement to adopt an initialization pose and losing track after a few frames. These problems have been addressed by the approaches proposed by Xbox\cite{Kinect2014} team which are capable of accurately predicting the 3D positions of body joints using single depth images without using any temporal information\cite{Shotton2013}. Understanding of human motion is not complete if the action of the human could not be inferred. In the survey by Microsoft research team\cite{KinectCV2013}, a background study on various algorithms used for human activity analysis is presented. Recently data-driven machine learning approaches like neural networks, support vector machines, clustering, decision trees and Bayesian networks have proven to be successful with recognition accuracy as high as 94\%\cite{Kinect2014}.\\
	
	The localization of humanoid robots is a challenging issue, due to rough odometry estimation, noisy onboard sensing, and the swaying motion caused by walking\cite{Cervera2012}. The Point cloud library\cite{RusuPCL11} which is one of the most widely used 3D perception software library, implements ready to use probabilistic tracking algorithms\cite{RUeda2012}. Studies on robot localization, obstacle mapping, and path planning in multilevel 3D environments by equipping Nao with a consumer-level depth camera has been reported in \cite{Maier2012}. Localization and motion planning in smart home environment using an external Kinect sensor have been proposed in \cite{Cervera2012}. These methods are computationally demanding and it could cause overall performance degradation particularly when we want to share the same sensor for both human motion recognition and localization of the robot. Tracking rectangular fiducial markers using  augmented reality tool-kits like ALVAR\cite{ALVAR} can be interesting if we could embed those markers on the humanoid robot. This is one of the simplest and cheapest solution in terms of the computational power as it can provide position and orientation of the physical markers in real time.\\
	
	The users of social robots do not have necessary backgrounds in programming and design of robot behaviors. The main challenge in the behavior design is the ability to define the behavior which can abstract complex data flows from the end user. There exists flow-chart based visual programming languages\cite{Choregraphe} which allow non-programmers to create robot applications using a set of pre-built behavioral blocks. These programs are very intuitive and allow the users to realize complex sequence of movements and sequential behaviors. However when it comes to designing reactive behaviors for human-in-the-loop scenarios, the existing visual programming methods increase the cognitive load on the end users. Specialized robot programming techniques like Task description language\cite{Simmons724883}, scripting techniques like Universal Robotic Body Interface(URBI)\cite{Baillie4814281} and middlewares ROS\cite{quigley2009ros} have been proposed in the literature. Though these programs provide modular and distributed architecture, support multiple sensors and robots etc., all these tools require high level of skill in robotics and programming to use them. Recently non-domain-specific solution like Target-Drives-Means is proposed in \cite{BerenzTDM2014}, however it lacks an intuitive interface.\\
	
	Nowadays there has been a lot of efforts to teach programming to children and people without computer science background\cite{Scratch}\cite{Blockly}. These tools are very intuitive and have already been proven to be used by novice programmers to build games and educational applications. The Blockly library\cite{Blockly} from Google offers a complete client side JavaScript library which could be used for developing custom blocks and code generators as per the application requirements.
%Hierarchical organization of behaviors and modularity are also being investigated \cite{Jaegeretal},\cite{Baldassarre:2013:CRM:2560111},\cite{hurdus4648045}
%\subsection{Behavior Design Frameworks}
\section{Motion Driven Behavior Interface}
%
We propose a light weight interface for designing human motion driven behaviors taking inspiration from distributed architecture\cite{quigley2009ros} and intuitive visual programming techniques\cite{Blockly}. 
%
\subsection{System Architecture}
%
The important components of the system shown in Fig~\ref{fig:architecture} are
\begin{itemize}
\item \textbf{Application Components}
\begin{itemize}
\item \emph{Context}: The application context contains the complete description of the world. It contains latest information about all the robots including their location, sensor data and status. It also contains information about all the humans in the environment along with their active motions/gestures as supplied by the motion recognition modules.
\item \emph{Parameter Server}: The parameter server acts as a central repository for managing the parameters of the system and of the distributed components.
\item Embedded Web Server: The web server embedded in the application serves the file and data requests from the web client.
\item \emph{Context Orchestrator}: The orchestrator collect uptodate information about the robots and humans in the environment from the perception system and updates the Context.
\item \emph{Bootstrapper} : The bootstrapper takes care of initializing the system and starting up all the pre-configured nodes. It also takes of starting and stopping the behavior programs when requested by the user. 
\item \emph{Behavior Program} : A dynamic component that will be created when the user starts the program he/she designed using the user interface. The declarative description of the behavior is parsed in order to create a memory model. The Behavior program node monitors the application context for the motion triggers and invokes the corresponding robot actions according to the way it is being described in the program.
\end{itemize}
\item \textbf{Distributed Components} : These are nodes in the system each with a specific goal that can be started/stopped at any time during the entire application life-cycle without affecting the other nodes or the system. All the nodes will communicate with the application using message passing techniques. They can run in any machine inside the network.
\begin{itemize}
\item \emph{Motion Recognition Node} : A dedicated node that interacts with a motion recognition sensor and sends the detected gestures and motions to the application. Additionally each motion recognition module registers a set of actions/gestures that could be detected with the sensor associated with it.
\item \emph{Robot Interface Node} : A dedicated node that interacts with a specific robot and can invoke a set of actions on it. It also sends periodic update about the Robot status to the application. Moreover it registers a set of actions that could be invoked on the robot associated with it.
\item \emph{Localization Node} : A dedicated node which uses the perception system to resolve and publish the current position of the robot and the human which is very important for interaction.
\end{itemize}
\item \textbf{User Interface}: The user interface is a web application that runs on any latest web-kit browsers supporting WebGL technology. 
\begin{itemize}
\item \emph{Behavior Designer}: The Behavior designer surface could be used by the user to drag and drop the behavior blocks and construct the program using the set of motion capabilities registered by the active motion recognition nodes and the set of robot action capabilities registered by the active robot interface nodes. The behavior designed using the designer will be encoded into a declarative XML format and sent to the server when the user request to start the program. The designer offers a full range of capabilities like Create/Edit/Delete/Save behavior programs.
\item \emph{Visualization}: The visualization could be used to see the interaction of the human and robot inside a virtual 3D environment.
\end{itemize}
\end{itemize}
\begin{figure}
\centering
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{../thesis/assets/system_setup.eps}
\caption[System Setup]{System Setup}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{../thesis/assets/architecture.eps}
\caption[System Architecture]{Architecture}
\end{subfigure}
\caption[System Architecture]{System Architecture}
\label{fig:architecture}
\end{figure}
%
\subsection{Behavior Program}
%
The behavior program is structured in a simple way so that it could be easily understood by the end user. The behavior program whose conceptual model shown in Fig.~\ref{fig:program_concept} is composed of
\begin{itemize}
\item Startup Block: The start up block will be executed once when the user starts the program. The user can add a set of actions to be performed when the program starts. This block is optional and there cannot be more than once block of this kind in a program.
\item Behavior Block: The behavior block is composed of
\begin{itemize}
\item A \textbf{trigger} that activates this block. The trigger is a human motion gesture parameterized by the gesture confidence value.
\item The \textbf{lifetime} of each behavior block could be configured to run only once, forever or until a condition is met. 
\item The \textbf{priority} of the block could be set to low, normal or high and the execution is done based on Fixed-priority pre-emptive scheduling. If a higher priority behavior request to use a resource (i.e. robot) which is being used by a lower priority behavior, the higher priority preempts the lower priority behavior.
\item Similar to the behavior program level, at each behavior block level as set of \textbf{startup} and \textbf{exit} actions could be set which would be executed only once during the creation and termination respectively. The \textbf{cyclic} actions will be performed each time the trigger condition is met.
\end{itemize}
\item Exit Block: The exit block similar to start up block will be executed once when the lifetime of all the configured behaviors expire. This block is optional as well and there could be only block of this kind in a program.
\end{itemize}
The block level implementation of the conceptual model is shown in Fig.~\ref{fig:program_blocks}
\begin{figure}
\centering
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{../thesis/assets/program_structure.eps}
\caption[Conceptual Model]{Conceptual Model}
\label{fig:program_concept}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{../thesis/assets/program_block.png}
\caption[Block Implementation]{Block Implementation}
\label{fig:program_blocks}
\end{subfigure}
\caption[Program Structure]{Program Structure}
\label{fig:program}
\end{figure}
%
\subsection{Experimentatal Setup}
%
The entire framework has been implemented using open network communication standards powered by ZeroMQ\cite{ZeroMQ} and message serialization is done using the Google Protocol Buffers\cite{ProtocolBuffers} which is a language-neutral, platform-neutral, extensible mechanism for serializing structured data. The Kinect for Windows is used as the motion capture system. A set of gestures are created using the Visual Gesture Builder that comes with the Kinect Sdk\cite{Kinect2014}. The process involves capturing the clips of the motions, tagging the clips with the appropriate gestures and training the gesture recognizer. The trained gestures are then exported as a Visual gesture database and integrated with the motion recognition node. The humanoid robot NAO powered by the latest NaoQi OS V2.1.3 is used for evaluating the interaction scenarios created using behavior design interface.  A set of actions of the Nao humanoid robot are developed as python scripts that are being named corresponding to the actions. The Robot interface node is responsible for invoking these actions whenever requested. The localization of the humanoid robot is performed by combining the marker detection using Augmented reality toolkit ALVAR\cite{ALVAR} and a simple 2-DOF kinematic model of the robot from the torso to the head. The software is tested on a 64-bit Intel Core i7 CPU with clock speed 3.60 Ghz and 8 GB of RAM on a  Microsoft Windows 8.1 operating system. The source code of the software is available as open source at \url{https://github.com/praveenv4k/ExPeriMot}
\begin{figure}
\centering
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{../thesis/assets/program_structure.eps}
\caption[Conceptual Model]{Conceptual Model}
\label{fig:scenario1_program}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{../thesis/assets/program_block.png}
\caption[Block Implementation]{Block Implementation}
\label{fig:scenario1_setup}
\end{subfigure}
\caption[Program Structure]{Program Structure}
\label{fig:program}
\end{figure}
\subsubsection{Scenario 1}
%
In this scenario we consider NAO robot as a demonstrator in a museum. The user would like to design this scenario where when a human comes into the vicinity of the robot, the robot would approach him and start introducing the history of the museum. Such a scenario could be easily designed using our behavior interface as shown in Fig~\ref{fig:scenario1_program}
%
\subsubsection{Scenario 2}
%
\begin{figure}
\centering
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{../thesis/assets/program_structure.eps}
\caption[Conceptual Model]{Conceptual Model}
\label{fig:scenario2_program}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{../thesis/assets/program_block.png}
\caption[Block Implementation]{Block Implementation}
\label{fig:scenario2_setup}
\end{subfigure}
\caption[Program Structure]{Program Structure}
\label{fig:program}
\end{figure}
A physiotherapist who is in a remote hospital would like to prepare a scenario where he wants the robot to give directions to the patient who is recovering from a fracture of his hand. The exercise includes the robot to interact with the patient and count the number of times he does the exercise as designed by the therapist. A reference implementation of such a scenario using our behavior interface is shown in Fig~\ref{fig:scenario2_program}
%
\section{Conclusions}
%
\begin{itemize}
\item Discussions
\begin{itemize}
\item Compromise between the simplicity and expressiveness
\item Not just social robotics but also industrial scenarios
\item Exploit the smart devices around and integrate to provide more rich interaction experience like personalization, robust estimation of the motion etc., 
\end{itemize}
\end{itemize}
For the moment we have evaluated our system only for the Kinect motion capture system working seamlessly with the Nao humanoid robot for a set of predefined gestures and robot actions. We are planning to develop an extensive database containing commonly encountered gestures and also an extensive set of primitive robot actions. These will be made available to the end user through the intuitive programming interface which could be then used for defining complex motion driven behaviors. Additionally we are also planning to integrate our system to work with other modes of motion recognition like IMU, Accelerometers and Gyroscopes that are available in smart-phones and wearable devices. Similarly we would like to integrate other system with other robots like Pepper which we expect to receive soon.
%
\section{Acknowlegdements}
%
		We would like to thank the members of GVLab of Tokyo University of Agriculture and Technology for helping us arranging the resources to realize this work and participate in the experimentation.
%
% ---- Bibliography ----
%
%\begin{thebibliography}{5}
%
\bibliographystyle{plain}
\bibliography{../Thesis/Bibliography}
\end{document}