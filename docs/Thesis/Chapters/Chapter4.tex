% Chapter Template

\chapter{System Implementation} % Main chapter title

\label{Chapter4} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 4. \emph{System Implementation}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title
The software framework proposed and introduced in \ref{Chapter3} has been exploited to develop the Indriya platform. In this chapter the software architecture, specific implementation details of each of the components in the system are described. The software is designed using a hierarchical structure. The main layers of the system are
\begin{itemize}
\item \emph{Hardware layer} is composed of sensors and robots connected to the network. Currently only those sensors that support TCP/IP network communication are considered.
\item \emph{Distributed components layer} is composed of individual processes that could be running across different computers/devices in the network
\item \emph{Application components layer} is the central server of the Indriya platform which contains the backend processing units for the application to run as a whole.
\item \emph{User interface layer} takes the responsibility of providing useful and intuitive user-interface to the end users.
\end{itemize}
The architecture of the complete system is shown in Fig.~\ref{fig:indriya_arch}. More finer details of each of the layers are presented in the following Sections~\ref{ssec:app_comp}$\sim$\ref{ssec:ui_comp}.
\begin{figure}
\centering
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{../thesis/assets/system_setup.eps}
\caption[System setup]{System setup}
\label{fig:system_setup}
\end{subfigure}
\begin{subfigure}[t]{0.44\textwidth}
\includegraphics[width=\textwidth]{../thesis/assets/indriya_architecture.eps}
\caption[Software architecture]{Software architecture}
\label{fig:architecture}
\end{subfigure}
\caption[Indriya architecture]{Indriya architecture}
\label{fig:indriya_arch}
\end{figure}
\section{Application components}
\label{ssec:app_comp}
The application level components are responsible for bootstrapping and maintain the uptodate status of the system. 
\subsection*{Application Context}
The application context contains the complete description of the world. It contains latest information about
\begin{itemize}[leftmargin={1cm},topsep={0pt},itemsep={0pt},partopsep={0pt},parsep={0pt}] 
\item Robots: A list of robots in the environment along with their 6D pose, Sensor information like joint values etc.,
\item Humans: A list of humans with their skeleton positions and orientation, active gestures/motions
\item Objects: A list of manipulable objects in the environment along with their properties like description, color etc.,
\item Gesture modules : A set of gesture recognition modules registered in the system that can actively provide information about the gestures of the humans in the environment.
\item Speech modules : A set of speech recognition modules registered in the system that can actively provide information about human voice commands.
\item Robot behavior modules: A set of robot behavior execution modules registered in the system that can invoke actions on a specified robot.
\end{itemize}
The schematic representation of data structure of the context is shown in Fig~\ref{fig:system_context}
\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{assets/context_class.eps}
\caption[Application context]{Application context}
\label{fig:system_context}
\end{figure}
\subsection*{Parameter server} 
The parameter server acts as a central repository for managing the parameters of the system and of the distributed components. The parameter server is basically designed as a responder socket (request-response protocol) which responds to the requests from a remote client. The list of supported request to pull the data from the parameter server as shown in the Table~\ref{table:parameter_server}
\begin{table}[H]
\centering
\small
\caption{Parameter server interface specification}
\label{table:parameter_server}
\begin{tabular}{| p{6.6cm} | p{3.5cm} | p{3cm} |}
\hline
  \textbf{Request Description} & \textbf{Request Arguments} & \textbf{Response}
  \tabularnewline \hline
  Node Parameters request & 1. Name of the node & Node Parameters \\
  {}                      & 2. Node Parameters & {}
                                          \tabularnewline\hline
                                          
  Motion recognition module registration request &  1. Name of the node & Registration Status   \\
  {}                                             &  2. Module Information & {(Success/Failure)}
                                          \tabularnewline\hline

  Speech recognition module registration request &  1. Name of the node & Registration Status   \\
  {}                                             &  2. Module Information & {(Success/Failure)}
                                          \tabularnewline\hline
  
  Robot Interface module registration request &  1. Name of the node & Registration Status   \\
  {}                                             &  2. Module Information & {(Success/Failure)}
                                          \tabularnewline\hline
  File Request &   1. Name of the file & Contents of the file
  										 \tabularnewline\hline
\end{tabular}
\end{table}
% \begin{table}[H]
% \centering
% \small
% \caption{Parameter server interface specification}
% \label{table:parameter_server}
% \begin{tabular}{| p{5cm} | p{4cm} | p{2.8cm} |}
% \hline
%   \textbf{Request Description} & \textbf{Request Arguments} & \textbf{Response}
%   \tabularnewline \hline
%   Node Parameters request & \begin{itemize}[leftmargin=*,topsep={0pt},itemsep={0pt},partopsep={0pt},parsep={0pt}] 
%                                                   \item Name of the node
%                                                   \end{itemize} & Node Parameters 
%                                           \tabularnewline\hline
                                          
%   Motion recognition module registration request &  \begin{itemize}[leftmargin=*,topsep={0pt},itemsep={0pt},partopsep={0pt},parsep={0pt}] 
%                                                   \item Name of the node
%                                                   \item Module Information
%                                                 \end{itemize} & Registration Status  (Success/Failure)
%                                           \tabularnewline\hline

%   Speech recognition module registration request &  \begin{itemize}[leftmargin=*,topsep={0pt},itemsep={0pt},partopsep={0pt},parsep={0pt}] 
%                                                   \item Name of the node
%                                                   \item Module Information
%                                                 \end{itemize} & Registration Status  (Success/Failure)
%                                           \tabularnewline\hline
  
%   Robot Interface module registration request & \begin{itemize}[leftmargin=*,topsep={0pt},itemsep={0pt},partopsep={0pt},parsep={0pt}] 
%                                                 \item Name of the node
%                                                 \item Module Information
%                                             \end{itemize} & Registration Status  (Success/Failure)
%                                           \tabularnewline\hline
%   File Request & \begin{itemize}[leftmargin=*,topsep={0pt},itemsep={0pt},partopsep={0pt},parsep={0pt}] 
%                                                   \item Name of the file
%                                                   \end{itemize} & Contents of the File  
%                        \tabularnewline\hline
% \end{tabular}
% \end{table}
\subsection*{Context orchestrator} The orchestrator collects uptodate information about the robots and humans in the environment from the perception system and updates the Application context. The algorithm used by the orchestrator to update the context information is shown in Algorithm~\ref{alg:context_sync}.

\begin{algorithm}[H]
 \KwData{(CONFIG: Configuration XML, CONTEXT: Application context, TIME\_OUT)}
 \textbf{\emph{Init}}:\\
 \quad PUBLISHERS = READ\_ALL\_PUBLISHER\_INFO(CONFIG)\;
 \quad SUBSCRIBERS = CREATE\_SUBSCRIBERS(PUBLISHERS)\;
 \While{run}{
 	\ForAll{SUBSCRIBER in SUBSCRIBERS}{ 
 		DATA = RECEIVE\_DATA(SUBSCRIBER,TIME\_OUT)\;
 		UPDATE\_CONTEXT(CONTEXT, DATA)\;
 	}
 }
 \caption{Context synchronization algorithm}
 \label{alg:context_sync}
\end{algorithm}
\subsection*{Embedded web server}
The web server embedded in the application is responsible for data and file transaction between the Indriya system and the web client. The server side implements a set of RESTful services \cite{RESTFul} which could be accessed from the web clients. The JSON data format which is the default standard for modern web application development has been used as data exchange format between the server and the web clients. The list of RESTful API implemented on the server side is shown in Table~\ref{table:restful_api}
\begin{table}[H]
\centering
\small
\caption{Embedded web server RESTful API}
\label{table:restful_api}
\begin{tabular}{|l|p{2.8cm}|p{1.2cm}|p{5.5cm}|}
\hline
  \textbf{Request Url}  & \textbf{Parameters} & \textbf{Request Type} & \textbf{Description/Response}
  \tabularnewline \hline
  /models/{type}/(?$<$all$>$.*) & File Name & GET & Robot 3D Model Data
                                          \tabularnewline\hline
                                          
  /context  & - & GET & Current context data  
  										                    \tabularnewline\hline
  										 
  /robot  &  - & GET & Default Robot information  
  										                    \tabularnewline\hline										 
  
  /humans & - & GET  & Information about all the human in the environment  
                                          \tabularnewline\hline
                                          
  /human/\{id\} & id - Human Id & GET  & Information about the human with the given id.  
                                          \tabularnewline\hline
                                          
  /jointvals & - & GET  & Joint values of the default robot  
                                          \tabularnewline\hline
                                          
  /visualize/skeleton/list & - & GET  & 3D Skeleton positions of all the humans in the environment  
                                          \tabularnewline\hline                                        
       
  /designer/program/list & - & GET  & List of all the behavior programs stored in the server  
                                          \tabularnewline\hline                                                               
                                          
  /designer/program/start & Behavior Program & POST  & Request the bootstrapper to start the program
  										                    \tabularnewline\hline   

  /designer/program/save & File Name, Behavior Program & POST  & Request the bootstrapper to start the program
                                          \tabularnewline\hline 
  /designer/program/stop & - & POST  & Request to stop running program if any
                                          \tabularnewline\hline
\end{tabular}
\end{table}
\subsection*{Bootstrapper} 
The bootstrapper takes care of initializing the system and starting up all the pre-configured nodes. The configuration information of the nodes is specified using the XML configuration file described in Section~\ref{sec:config_file}. It also takes care of starting and stopping the behavior programs when requested by the user. The sequence of operations bootstrapper performs is described in Algorithm~\ref{alg:bootstrapper}.

\begin{algorithm}[H]
 \KwData{(XML Config File)}
 \textbf{\emph{Startup}}:\\
 \quad INIT\_AND\_RUN\_PARAMETERSERVER()\;
 \quad INIT\_AND\_RUN\_CONTEXT\_ORCHESTRATOR()\;
 \quad INIT\_AND\_RUN\_CONTEXTSERVER()\;
 \quad INIT\_AND\_RUN\_WEBSERVER()\;
 \quad NODES = GET\_NODES(XML Config File)\;
 \quad PROCESSES = []\;
 \ForAll{NODE in NODES}{ 
 	\If{NODE is ENABLED}{
 		PROCESS = CREATE\_PROCESS(NODE)\;
 		APPEND(PROCESSES, PROCESS)
 	} 
 }
 \While{run}{
 	MONITOR(PROCESSES)
 }
 \textbf{\emph{Shutdown}}:\\
 \quad SHUTDOWN\_WEBSERVER()\; 
 \quad SHUTDOWN\_CONTEXTSERVER()\;
 \quad SHUTDOWN\_CONTEXTSYNC()\;
 \quad SHUTDOWN\_PARAMETERSERVER()\;
 \quad SHUTDOWN(PROCESSES)
 \caption{Bootstrapper Algorithm}
 \label{alg:bootstrapper}
\end{algorithm}
\section{Distributed components}
\label{ssec:dist_comp}
These are nodes in the system each with a specific goal that can be started/stopped at any time during the entire application life-cycle without affecting the other nodes or the system.
\subsection{Motion recognition node} A dedicated node that interacts with a motion recognition sensor and sends the detected gestures and motions to the application. Additionally each motion recognition module registers a set of actions/gestures that could be detected with the sensor associated with it. The gesture recognition logic depends on the sensor associated with the gesture recognition module. The gesture recognition workflow of kinect based system in explained in Section~\ref{sssec:kinect_gestures}
\subsubsection{Kinect gesture recognition}
\label{sssec:kinect_gestures}
The Kinect gesture recognition system involves a training phase during which the desired gestures are captured and tagged. These tagged gestures will be used by a gesture detector trainer which will generate a set of training examples. The training results are stored in files and will be used by the gesture detector to perform per-frame classification of the data. The Visual Gesture Builder (VGB) tool that comes together with Kinect for Windows SDK eases the process of tagging the gestures, evaluating the gesture recognition and creating the gesture database. The work-flow of using the VGB is shown in the Fig.~\ref{fig:vgb_workflow}.
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{assets/vgb_flow.eps}
\caption[Visual Gesture Builder work flow]{Visual Gesture Builder work flow \cite{KinectSDK2014}}
\label{fig:vgb_workflow}
\end{figure}
The process followed for generating the Left hand wave gesture is shown in the Fig.~\ref{fig:gesture_waveleft}. As shown in the figure at first the clips are captured using the Kinect studio tool and stored as \textbf{XEF} files. The clips are then imported in the Visual gesture builder. The imported clips is traversed and the tagging is done (i.e at each frame it is possible to specify if that particular frame is part of the gesture of interest or not). This process could be repeated on one or more imported clips. The tagged clips are built in order to generate a set of weak classifiers and learn confidence values for each of those classifiers. Once the training process is complete, the trained classifier could be evaluated over a set of test clips using a live preview as shown in step 3 in the figure. When the results are satisfactory, the training data could be exported as a database saved as \textbf{VGD} (Visual Gesture Database) format.
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{assets/gesture_recog_flow.eps}
\caption[Left hand wave gesture work flow]{Left hand wave gesture work flow}
\label{fig:gesture_waveleft}
\end{figure}
The database is imported into the gesture recognition module and the responsibility of this module is to notify any interested subscribers when a gesture is recognized. Additionally this module also publishes information about the skeletons of the humans in view. The algorithm of this module is shown in Algorithm~\ref{alg:gesture_recognize}

\begin{algorithm}[H]
 \KwData{Gesture database}
 \KwResult{Gesture Triggers}
 \textbf{\emph{Init}}:\\
 \quad gestures = READ\_DATABASE()\;
 \quad REGISTER\_GESTURES(gestures)\;
 \While{True}{
  skeletons = GET\_SKELETONS()\;
  \ForAll{gesture in gestures}{ 
    detected = DETECT\_GESTURE(gesture)\;
    \If{detected}{
      PUBLISH(gesture)\;
    } 
  }
  PUBLISH(skeletons)\;
 }
 \caption{Kinect gesture recognition module}
 \label{alg:gesture_recognize}
\end{algorithm}
\subsection{Speech recognition node}
The Nao humanoid robot already comes with an inbuilt speech recognition and sythesis ability. Since Indriya system is designed to support different robots, it has been decided not to use already existing speech capabilities of Nao robot. Instead the Microsoft speech platform 11 is used for recognizing the speech input. The Kinect for Windows SDK includes a custom acoustical model that is optimized for the Kinect's microphone array and supports recognition of several languages. The grammar files for some commonly used commands like GO, STOP, SIT, STAND, Hello etc., were created to be used for the actual interaction scenarios. The speech recognition module algorithm is shown in Algorithm~\ref{alg:speech_recognize}.

\begin{algorithm}[H]
 \KwData{language}
 \KwResult{Speech Triggers}
 \textbf{\emph{Init}}:\\
 \quad grammar = LOAD\_GRAMMAR(language)\;
 \quad recognizer = LOAD\_RECOGNIZER(language)\;
 \quad INIT\_SPEECH\_ENGINE(recognizer, grammar)\;
 \quad REGISTER\_SPEECH\_MODULE(grammar)\;
 \While{True}{
    \If{recognized}{
      PUBLISH(recognized\_command)\;
    } 
 }
 \caption{Kinect speech recognition module}
 \label{alg:speech_recognize}
\end{algorithm}

\subsection{Robot interface node} 
The Robot interface node is a dedicated node that interacts with a specific robot and can invoke a set of actions on it.  It registers a set of actions that could be invoked on the robot associated with it. The robot interface node sets up a publisher that periodically sends the information about the robot sensor information like joint values to the application. Additionally it sets up a responder socket which will wait for action execution request from the remote node. 

The robot interface node implements a Finite state machine (FSM) shown in Fig.~\ref{fig:robot_fsm} in order to appropriately streamline the data/status management with the remote caller node. The states of the FSM are
\begin{itemize}[leftmargin={1cm},topsep={0pt},itemsep={0pt},partopsep={0pt},parsep={0pt}] 
\item \emph{idle:} The robot is ready to receive new execution request.
\item \emph{wait:} The node is waiting for the action execution arguments.
\item \emph{error:} The node/robot has encountered an error during execution.
\item \emph{ready:} The robot is waiting for the run trigger.
\item \emph{run:} The robot is executing the requested task.
\item \emph{stop:} The robot is stopping execution
\end{itemize}
\begin{figure}[H]
\centering
\begin{tikzpicture}[->,shorten >=1pt,auto,node distance=4cm,thick]
%\tikzstyle{every state}=[fill=gray,draw=none,text=black]
\tikzstyle{every state}=[circle,minimum size=1.5cm,text=black]
\node[initial,state] (A) {$idle$};
\node[state] (B) [above right of=A] {$wait$};
\node[state] (C) [below right of=B] {$error$};
\node[state] (D) [above right of=C] {$ready$};
\node[state] (E) [below right of=C] {$run$};
\node[state] (F) [below right of=A] {$stop$};
\path 
(A) edge [bend right] node {\textbf{on\_start\_req}} (B)
    %edge [loop below] node {} (A)
    
(B) edge node {\textbf{on\_args}} (D)
    edge [left,bend left] node {\textbf{on\_err}} (C)
    %edge [loop above] node {} (B)
    
(C) edge node {\textbf{on\_reset}} (A)
    %edge [loop above] node {1,0,R} (C)
    
(D) edge [bend left] node {\textbf{on\_run\_req}} (E)
  edge [right,bend right] node {\textbf{on\_err}} (C)
  %edge [loop above] node {1,1,R} (E)

(E) edge [bend left=70] node {\textbf{on\_task\_fin}} (A)
    edge node {\textbf{on\_stop\_req}} (F)
    edge [right,bend left] node {\textbf{on\_err}} (C)
    
(F) edge [left,bend right] node {\textbf{on\_err}} (C)
    edge  [bend right] node {\textbf{on\_stopped}} (A)  
    %edge [loop below] node {1,0,R} (E)
;
\end{tikzpicture}
\caption[Finite state machine of robot interface node]{Finite state machine of robot interface node}
\label{fig:robot_fsm}
\end{figure}
Each of the above states has a self loop when waiting for the state transition trigger. It is not shown in the figure for brevity. The events that causes the state transition in the FSM are
\begin{itemize}[leftmargin={1cm},topsep={0pt},itemsep={0pt},partopsep={0pt},parsep={0pt}] 
\item \emph{on\_start\_req:} Received start execution request
\item \emph{on\_args:} Received task execution arguments.
\item \emph{on\_run\_req:} Received run request.
\item \emph{on\_task\_fin:} Task execution completed
\item \emph{on\_stop\_req:} Received stop execution request. This is served with high priority.
\item \emph{on\_stopped:} The task stop operation completed
\item \emph{on\_reset:} Received error reset request
\item \emph{on\_err:} An error occured during the execution
\end{itemize}
The robot interface node dedicated to Nao humanoid robot is described in Section~\ref{sssec:nao_interface}
\subsubsection{NAO robot interface}
\label{sssec:nao_interface}
The Nao robot interface makes use of the NaoQi python SDK which provides API for accessing various functionalities of the robot such as joint/cartesian control, memory access, motion, speech, object/face/people detection and tracking, behavior invocation etc., A set of necessary functions are implemented in the NaoBehaviorModule and they are registered to the application when the node is started up. The control flow is shown in Fig.~\ref{fig:nao_interface}.
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{assets/NaoBehaviorModule.eps}
\caption[NAO robot Interface]{NAO robot Interface}
\label{fig:nao_interface}
\end{figure}
The visual representation of a set of action blocks developed for Nao humanoid robot can be found in Table~\ref{table:nao_blocks}
% \begin{center}
% \begin{tikzpicture}[scale=0.2]
% \tikzstyle{every node}+=[inner sep=0pt]
% \draw [black] (55.7,-14.6) circle (3);
% \draw (55.7,-14.6) node {$WAIT\_ARG$};
% \draw [black] (35.2,-3.8) circle (3);
% \draw (35.2,-3.8) node {$IDLE$};
% \draw [black] (35.2,-3.8) circle (2.4);
% \draw [black] (55.7,-38.7) circle (3);
% \draw (55.7,-38.7) node {$RUN\_READY$};
% \draw [black] (32.9,-48.5) circle (3);
% \draw (32.9,-48.5) node {$RUN$};
% \draw [black] (28.9,-25.4) circle (3);
% \draw (28.9,-25.4) node {$STOP$};
% \draw [black] (44.7,-25.4) circle (3);
% \draw (44.7,-25.4) node {$ERROR$};
% \draw [black] (57.005,-17.299) arc (22.31081:-22.31081:24.631);
% \fill [black] (57,-36) -- (57.77,-35.45) -- (56.85,-35.07);
% \draw (59.35,-26.65) node [right] {$ON\_ARGS$};
% \draw [black] (38.197,-3.731) arc (87.25861:37.17824:21.184);
% \fill [black] (54.06,-12.09) -- (53.98,-11.15) -- (53.18,-11.75);
% \draw (53.19,-5.63) node [above] {$START\_REQ$};
% \draw [black] (33.218,-51.471) arc (33.84033:-254.15967:2.25);
% \draw (25.82,-55.47) node [below] {$RUNNING$};
% \fill [black] (30.73,-50.56) -- (29.79,-50.59) -- (30.35,-51.42);
% \draw [black] (29.936,-48.053) arc (-102.41345:-263.47757:22.391);
% \fill [black] (32.21,-3.94) -- (31.35,-3.53) -- (31.47,-4.53);
% \draw (11.81,-25.01) node [left] {$TASK\_FIN$};
% \draw [black] (32.39,-45.54) -- (29.41,-28.36);
% \fill [black] (29.41,-28.36) -- (29.06,-29.23) -- (30.04,-29.06);
% \draw (30.18,-37.19) node [left] {$STOP\_REQ$};
% \draw [black] (27.383,-27.975) arc (-2.77501:-290.77501:2.25);
% \draw (22.64,-30.42) node [left] {$STOPPING$};
% \fill [black] (25.93,-25.76) -- (25.16,-25.22) -- (25.11,-26.22);
% \draw [black] (29.74,-22.52) -- (34.36,-6.68);
% \fill [black] (34.36,-6.68) -- (33.66,-7.31) -- (34.62,-7.59);
% \draw (31.28,-14.02) node [left] {$STOPPED$};
% \draw [black] (54.018,-41.181) arc (-38.28052:-95.20104:20.74);
% \fill [black] (35.86,-48.99) -- (36.61,-49.56) -- (36.7,-48.56);
% \draw (50.89,-47.95) node [below] {$RUN\_REQ$};
% \draw [black] (53.79,-36.39) -- (46.61,-27.71);
% \fill [black] (46.61,-27.71) -- (46.74,-28.65) -- (47.51,-28.01);
% \draw (50.75,-30.62) node [right] {$ON\_ERR$};
% \draw [black] (53.56,-16.7) -- (46.84,-23.3);
% \fill [black] (46.84,-23.3) -- (47.76,-23.09) -- (47.06,-22.38);
% \draw (45.79,-19.52) node [above] {$ON\_ERR$};
% \draw [black] (31.9,-25.4) -- (41.7,-25.4);
% \fill [black] (41.7,-25.4) -- (40.9,-24.9) -- (40.9,-25.9);
% \draw (36.8,-25.9) node [below] {$ON\_ERR$};
% \draw [black] (43.49,-22.65) -- (36.41,-6.55);
% \fill [black] (36.41,-6.55) -- (36.27,-7.48) -- (37.19,-7.08);
% \draw (40.68,-13.62) node [right] {$RESET$};
% \draw [black] (34.26,-45.83) -- (43.34,-28.07);
% \fill [black] (43.34,-28.07) -- (42.53,-28.56) -- (43.42,-29.01);
% \draw (39.49,-38.08) node [right] {$ON\_ERR$};
% \end{tikzpicture}
% \end{center}
\subsection{Localization node} A dedicated node which uses the perception system to resolve and publish the current pose of the robot.
\subsubsection{NAO robot localization}
The fiducial marker based approach is used in order to localize the robot in the field of view of the Kinect sensor. The ALVAR \cite{ALVAR} toolkit is used for marker detection purpose which relies on OpenCV library \cite{bradski2000opencv} for image processing. The OpenCV library cannot directly read the image streams from Kinect sensor and hence a OpenCV compatible driver is implemented to read the RGB image streams. The first step in the marker detection is to calibrate the camera in order to get its intrinsic parameters and the calibration parameters are stored in an XML file. A set of supported markers of known identifiers are chosen and a cube of known geometry is created (Table~\ref{table:cube_spec}). The cube is designed in such a way that at any instant atleast one of the markers will be detected if the robot is in the detectable range of the camera. The cube is mounted on the head of NAO robot since it will avoid the problem of occlusion in most cases. However since the head of the NAO has 2 DOF which will result in the movement of mounted cube even if the robot itself does not move. In order to overcome this problem, a kinematic model of the NAO humanoid robot from torso frame to tip of the head frame parameterized using Modified Denavit Hartenberg (MDH) \cite{khalil2004modeling} parameters is used. The MDH parameters are shown in Table~\ref{table:nao_mdh}. A world frame is represented using a marker and the setup is shown in Fig~\ref{fig:localize_setup}.
\begin{table}[H]
\centering
\caption{Marker cube specification}
\label{table:cube_spec}
\begin{tabular}{|r|l|}
\hline
  \textbf{Number of markers}  & 5
  \tabularnewline \hline
  \textbf{Size of markers}  & 6 cm
                                          \tabularnewline\hline
  \textbf{Size of cube}  & 8 cm
                                          \tabularnewline\hline
  \textbf{Maximum detection range}  & 3.6 m                                          
                                          \tabularnewline\hline                    
\end{tabular}
\end{table}
\begin{table}[H]
\centering
%\small
\caption{MDH parameters from torso to top of head}
\label{table:nao_mdh}
% \begin{tabular}{|l|l|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{3cm}|}
\begin{tabular}{|l|l|c|c|c|c|p{2cm}|}
\hline
  \textbf{$i$}  & \textbf{$j$}  & \textbf{$\alpha_j$ (rad)} & \textbf{$d_j$ (mm)} & \textbf{$\theta_j$ (rad)} & \textbf{$r_j$ (mm)} & \textbf{Description}
  \tabularnewline \hline
  - & 0 & 0 & 0 & 0 & 0 & Torso/Base
                                          \tabularnewline\hline
                                          
  0 & 1 & 0 & 0 & $\theta_1$ & 126.50 & Head Yaw
                                          \tabularnewline\hline
                       
  1 & 2 & $-\frac{\pi}{2}$ & 0 & $\theta_2$ & 0 & Head Pitch
                                          \tabularnewline\hline                    
  
  2 & 3 & $\frac{\pi}{2}$ & 0 & 0 & 110.00 & Head Tip
                                          \tabularnewline\hline
\end{tabular}
\end{table}

\subparagraph{Modeling}
For computing the absolute pose of the torso frame of the robot with respect to the world frame, the coordinate frames shown in Fig.~\ref{fig:nao_localization} are used.

\begin{tabular}{r l}
\centering
  Kinect Frame & $F_K$ \\ 
  Marker Frame & $F_M$ \\ 
  Torso Frame & $F_T$ \\ 
  World Frame & $F_W$ \\ 
  Transformation Matrices  & ($^{K}T_M$,$^{T}T_M$,$^{K}T_W$,$^{W}T_T$) \\
\end{tabular}

\begin{figure}[H]
\centering
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{../thesis/assets/localization_concept2.eps}
\caption[Modeling]{Coordinate Frames}
\label{fig:localize_frames}
\end{subfigure}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{../thesis/assets/localization_frames.png}
\caption[Actual setup]{Actual setup}
\label{fig:localize_setup}
\end{subfigure}
\caption[NAO robot localization]{NAO robot localization}
\label{fig:nao_localization}
\end{figure}

The algorithm for the computation given the marker parameters, calibration information of the camera, MDH parameters (Table~\ref{table:nao_mdh}) and the joint values ($\theta_1,\theta_2$) at each time instant is shown in Algorithm~\ref{alg:localize}.

\begin{algorithm}[H]
 \KwData{marker\_size, cube\_size, MDH\_Params}
 \KwResult{TORSO\_POSE}
 \textbf{\emph{Init}}:\\
 \quad g\_model := INIT\_GEOM\_MODEL(MDH\_Params)\;
 \quad m\_model := MARKER\_MODEL(marker\_size, cube\_size)\;
 \While{True}{
 	data = READ\_RGB\_STREAM()\;
  [$\theta_1$,$\theta_2$] = READ\_JOINT\_VALUES()\;
 	marker\_poses = DETECT\_CUBE\_MARKERS(data)\;
  $^{K}T_W$ = DETECT\_WORLD\_MARKER(data)\;
 	$^{K}{T}_{M}$ = TRANSFORM\_TO\_TOP\_FRAME(marker\_poses, m\_model)\;
 	$^{T}{T}_{M}$ = COMPUTE\_TOP\_FRAME(g\_model,$\theta_1$,$\theta_2$)\;
 	$^{K}{T}_{T}$ = $^{K}{T}_{M}$ $\times$ ${^{T}{T}_{M}}^{-1}$\;
  $^{W}{T}_{T}$ = ${^{K}{T}_{W}}^{-1}$ $\times$ ${^{K}{T}_{T}}$\;
 	TORSO\_POSE = MEDIAN\_FILTER($^{W}{T}_{T}$)\;
 	PUBLISH(TORSO\_POSE)\;
 }
 \caption{Localization algorithm}
 \label{alg:localize}
\end{algorithm}
\section{Behavior program} 
\label{ssec:behavior_program}
A dynamic component that will be created when the user starts the program he/she designed using the user interface. The Behavior program node monitors the application context for the motion/speech triggers and invokes the corresponding robot actions according to the way it is being described in the program.
\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{../thesis/assets/program_structure.eps}
\caption[Behavior program - Conceptual model]{Behavior program - Conceptual model}
\label{fig:program_concept}
\end{figure}
\subsection{Conceptual model}
The behavior program is structured in a simple way so that it could be easily understood by the end user. The conceptual model of behavior program is shown in Fig.~\ref{fig:program_concept}. The behavior program is composed of:
\begin{itemize}
\item \emph{Startup behavior}: The startup block will be executed once when the user starts the program. The user can add a set of actions to be performed when the program starts. The start-up block is optional and there cannot be more than one startup block in the program
\item \emph{Trigger behavior}: The trigger behavior block is the core component of the behavior program. The block could be activated by a trigger such as human gesture, speech or proximity of the human and the robot. There could be many behavior blocks in a program each of which could be configured with a trigger of interest. The properties/attributes of a behavior block is shown in Table~\ref{table:behavior_block}. 
\begin{table}[H]
\centering
\small
\caption{Trigger behavior block properties}
\label{table:behavior_block}
\begin{tabular}{|l|p{11cm}|}
\hline
  \textbf{Property} & \textbf{Description}
  \tabularnewline \hline
  
  Trigger & The signal that activates the behavior block. The trigger source could be either of human presence/absence, motion gesture, vicinity of human, verbal command or any boolean edge trigger.
                                          \tabularnewline\hline
                                          
  Lifetime & The lifespan of the behavior block. \begin{itemize}[leftmargin=*,topsep={0pt},itemsep={0pt},partopsep={0pt},parsep={0pt}]
                                                                      \item Once: The block will be executed only once.
                                                                      \item Forever: The block will be executed forever each time trigger condition is met.
                                                                      \item Until: The block will be executed until a condition is met
                                                                   \end{itemize}
                                          \tabularnewline\hline
  
  Priority & The priority of the block could be Low, Normal or High and the behavior execution is scheduled based on fixed-priority preemptive scheduling. There is not dynamic priority allocation as the concept could be confusing for naive users.
                                          \tabularnewline\hline

  Startup & Tasks/actions to be performed once when the block is initialized
                                          \tabularnewline\hline

  Cyclic & Tasks/actions to be performed each time the trigger condition is met
                                          \tabularnewline\hline
  
  Exit &  Tasks/actions to be performed once when the lifetime of the block expires
                                          \tabularnewline                                                                       
                      \hline
\end{tabular}
\end{table}
\item \emph{Exit behavior}: The exit block will be executed once when the lifetime of all the configured behavior blocks expire. Like the startup block there could be only one exit block in the program.
\end{itemize}
\subsection{Block level implementation}
\label{sec:block_impl}
The visual blocks corresponding to each of the startup, trigger and exit behaviors are implemented. The complex programs could be composed by putting together these blocks. The startup and exit blocks are shown in Fig.~\ref{fig:blocks_init}. As could be seen from the figure, the blocks are quite simple making it possible to put the list of statements consisting of initialization and termination logic of the program. 
\begin{figure}[H]
%\centering
\begin{subfigure}[t]{0.28\textwidth}
\includegraphics[width=\textwidth]{../thesis/assets/blocks_init.png}
\caption[Startup/Exit blocks]{Startup/Exit blocks}
\label{fig:blocks_init}
\end{subfigure}
\begin{subfigure}[t]{0.33\textwidth}
\includegraphics[width=\textwidth]{../thesis/assets/blocks_behavior1.png}
\caption[Trigger behavior block 1]{Trigger behavior block 1}
\label{fig:trigger_block1}
\end{subfigure}
\begin{subfigure}[t]{0.33\textwidth}
\includegraphics[width=\textwidth]{../thesis/assets/blocks_behavior2.png}
\caption[Trigger behavior block 2]{Trigger behavior block 2}
\label{fig:trigger_block2}
\end{subfigure}
\caption[Behavior program constructs]{Behavior program constructs}
\label{fig:behavior_blocks}
\end{figure}
The trigger behavior block is designed in such a way that it incorporates the conceptual model and at the same time easier for the end user to understand and use it. The trigger behavior blocks are shown in Fig.~\ref{fig:trigger_block1} and \ref{fig:trigger_block2}. The visual design provides ability to compose the blocks by putting an external boolean trigger, choose priority and lifetime from the combo box interface, add startup,cyclic and exit statements etc.,
% \begin{figure}[H]
% \centering
% \begin{subfigure}[t]{0.38\textwidth}
% \includegraphics[width=\textwidth]{../thesis/assets/blocks_behavior1.png}
% \caption[Example 1]{Priority: Low, Execute: Once}
% \label{fig:trigger_block1}
% \end{subfigure}
% \begin{subfigure}[t]{0.38\textwidth}
% \includegraphics[width=\textwidth]{../thesis/assets/blocks_behavior2.png}
% \caption[Example 2]{Priority: High, Execute: Until}
% \label{fig:trigger_block2}
% \end{subfigure}
% \caption[Trigger behavior]{Trigger behavior}
% \label{fig:blocks_trigger}
% \end{figure}
The block interface is reactive in the sense that for instance when the lifetime of the block is switched from \emph{Once/Forever} to \emph{Until}, a new input appears in the block where the user can add the condition to stop the block.

\subsection{Code generation and execution}
The next step in the behavior program design flow is to generate the executable code from the blocks and execute them. Each blocks developed as part of the platform has an associated code generation module. So when the user decides to run a program, as a first step all the blocks in the workspace (the surface on which the program is designed) are iterated and code is generated. The blockly editor supports code generation for programming languages like JS, Python and Dart. However a custom code generator that could generate C\# code from the blocks has been used in Indriya. This is because most of the platform is written in C\# and there are loads of goodies that comes with the .NET framework. Traditionally there is a need to compile the C\# code and generate binaries when one wants to execute it. Thanks to the latest compiler technologies and tools like ScriptCS \cite{ScriptCS}, it is now possible to run the C\# code like scripts. This is exploited in the platform to dynamically generate the code and run.
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{../thesis/assets/execution_flow.eps}
\caption[Behavior program - Execution flow]{Behavior program - Execution flow}
\label{fig:program_execution}
\end{figure}

\begin{algorithm}[H]
 \KwData{program : Behavior program}
 startup := STARTUP\_BEHAVIORS(program)\;
 exit := EXIT\_BEHAVIORS(program)\;
 triggered := TRIGGER\_BEHAVIORS(program)\;
 sorted := SORT\_PRIORITY(triggered)\;
 \If{startup}{
  EXECUTE\_BEHAVIOR(startup)\;
 }
 \While{behavior lifetime}{
  \ForAll{behavior in sorted}{
    \If{behavior not complete}{
      \If{CHECK\_TRIGGER(behavior)}{
        current = GET\_ACTIVE\_TASK()\;
        priority1 = PRIORITY(behavior)\;
        priority2 = PRIORITY(current)\;  
        \If{priority1 $>$ priority2}{
          DO\_PREMEPTION(current)\;
          SCHEDULE\_TASK(behavior)\;
        }
        \ElseIf{no active task}{
          SCHEDULE\_TASK(behavior)\;
        }
      }
    }
  }
 }
 \If{exit}{
  EXECUTE\_BEHAVIOR(exit)\;
 }
 \caption{Behavior execution algorithm}
 \label{alg:behavior_engine}
\end{algorithm}
 The execution flow of the behavior program is shown in Fig.~\ref{fig:program_execution}. The generated behavior program is consumed by a \emph{Behavior execution engine} which dynamically identifies the configured behavior blocks. The main functionality of the behavior execution engine is to iterate and check if the trigger condition of the behavior block is met and once the condition is met, the execution of the corresponding behavior block is scheduled on a separate CPU thread. The behavior program accesses the uptodate information about the environment through the interface named the \emph{Behavior execution context} which acts as a proxy to the \emph{Application context}. The algorithm of the behavior execution by the engine is shown in Algorithm~\ref{alg:behavior_engine}.
\section{User interface}
\label{ssec:ui_comp}
The user interface is a web application that runs on any latest web-kit browsers supporting HTML5, CSS3 and WebGL technologies. The list of libraries and components used to build the web interface has been described in Section~\ref{sec:ui_design}. In this section the various screens in the developed user interface shown in Fig~\ref{fig:behavior_designer} has been described. 
\begin{figure}
\centering
\begin{tikzpicture}
    \node [anchor=north] (main) at (1.5,9.5) {\Large Main Tabs};
    \node [anchor=north] (cmd) at (5,9.5) {\Large Designer commands};
    \node [anchor=south] (toolbox) at (1.5,-1) {\Large Toolbox};
    \node [anchor=north] (name) at (10,9.5) {\Large Program name};
    \node [anchor=south] (surface) at (8,-1) {\Large Designer Surface};
    \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=\textwidth]{ui_designer.png}};
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
        %\draw[red,ultra thick,rounded corners] (0.62,0.65) rectangle (0.78,0.75);
        \draw[red,ultra thick,rounded corners] (0,0.93) rectangle (0.22,0.99);
        \draw[red,ultra thick,rounded corners] (0,0.89) rectangle (0.25,0.84);
        \draw[green,ultra thick,rounded corners] (0,0.825) rectangle (0.23,0.01);
        \draw[yellow,ultra thick,rounded corners] (0.4,0.87) rectangle (0.6,0.93);
        \draw[blue,ultra thick,rounded corners] (0.235,0.825) rectangle (0.97,0.01);

        % \draw [-latex, ultra thick, red] (main) to[out=0, in=-120] (0.01,0.96);
        % \draw [-latex, ultra thick, red] (cmd) to[out=0, in=-120] (0.01,0.89);
        % \draw [-latex, ultra thick, green] (toolbox) to[out=0, in=-120] (0.05,0.5);
        % \draw [-latex, ultra thick, yellow] (name) to[out=0, in=-120] (0.5,0.87);
        % \draw [-latex, ultra thick, blue] (surface) to[out=0, in=-120] (0.5,0.5);
        \draw [-stealth, line width=3pt, red] (main) -- ++ (-0.0,-0.15);
        \draw [-stealth, line width=3pt, red] (cmd) -- ++ (-0.12,-0.24);
        \draw [-stealth, line width=3pt, green] (toolbox) -- ++ (0,0.5);
        \draw [-stealth, line width=3pt, yellow] (name) -- ++ (-0.1,-0.19);
        \draw [-stealth, line width=3pt, blue] (surface) -- ++ (0,0.5);
    \end{scope}
\end{tikzpicture}
    \caption[Behavior designer]{Behavior designer}
    \label{fig:behavior_designer}
\end{figure}
% The UI is composed of tabbed interface consisting of 3 main components as shown in Fig~\ref{fig:behavior_designer}.
% \begin{itemize}
% \item Design: To design the behavior
% \item Visualize: To visualize the interaction
% \item Monitor: To monitor the human and robot information
% \end{itemize}
\begin{table}[htbp]
\centering
\small
\caption{Trigger, Speech and Miscellaneous blocks}
\label{table:multi_blocks}
\begin{tabular}{|r|p{12cm}|}
    \hline
    \multicolumn{2}{|c|}{Trigger blocks}\\
    \hline
     {} & {} \\
                             Visual & \parbox[c]{1em}{\includegraphics[width=4cm]{../thesis/assets/blocks/triggers_gesture.png}} \\
                             Name   & Gesture trigger \\
                             Description & The block that detects the human gesture and generates the trigger \\
                             Parameters & 1. Gesture to be detected (WaveLeft, WaveRight, LiftLeft, LiftRight, Seated etc.,) \\ 
    \hline
    {} & {} \\
                             Visual & \parbox[c]{1em}{\includegraphics[width=8cm]{../thesis/assets/blocks/triggers_speech.png}} \\ 
                             %\parbox[r]{1em}{\includegraphics[width=4cm]{../thesis/assets/blocks/triggers_speech2.png}} \\
                             Name   & Voice trigger \\
                             Description & The block that detects the predefined voice commands and generates the trigger \\
                             Parameters & 1. Voice command to be detected (Go, Stop, Good bye, Come here, Help etc.,) \\
    \hline
% \end{tabular}
% \end{table}
% \begin{table}
% \centering
% \small
% \caption{Speech blocks}
% \label{table:speech_blocks}
% \begin{tabular}{|r|p{12cm}|}
    \hline
    \multicolumn{2}{|c|}{Speech blocks}\\
    \hline
     {} & {} \\
                             Visual & \parbox[c]{1em}{\includegraphics[width=3cm]{../thesis/assets/blocks/speech_wait.png}} \\
                             Name   & Speech response block \\
                             Description & Waits for the speech response from human and performs actions corresponding to the response \\
                             Parameters & 1. List of actions to be performed when the response corresponds to choice 1 \\
                             {}         & 2. List of actions to be performed when the response corresponds to choice 2 \\
    \hline
% \end{tabular}
% \end{table}
% \begin{table}
% \centering
% \small
% \caption{Miscellaneous blocks}
% \label{table:misc_blocks}
% \begin{tabular}{|r|p{12cm}|}
    \hline
    \multicolumn{2}{|c|}{Miscellaneous blocks}\\
    \hline
     {} & {} \\
                             Visual & \parbox[c]{1em}{\includegraphics[width=5cm]{../thesis/assets/blocks/console_print.png}} \\
                             Name   & Console print \\
                             Description & To print a debugging message in the terminal during program execution \\
                             Parameters & 1. Message to print \\ 
    \hline
    {} & {} \\
                             Visual & \parbox[c]{1em}{\includegraphics[width=4.5cm]{../thesis/assets/blocks/sleep.png}} \\
                             Name   & Sleep \\
                             Description & To suspend execution for a specified interval of time \\
                             Parameters & 1. Interval in milliseconds \\
    \hline
    {} & {} \\
                             Visual & \parbox[c]{1em}{\includegraphics[width=2.5cm]{../thesis/assets/blocks/parallel.png}} \\
                             Name   & Parallel execution block \\
                             Description & To perform two tasks simulataneously. Care should be taken that the action is not performed on the same robot on both the branches of the block \\
                             Parameters & 1. List of actions to be performed on robot 1 \\
                             {}         & 2. List of actions to be performed on robot 2 \\

    \hline
\end{tabular}
\end{table}
\begin{table}
\centering
\small
\caption{Nao blocks}
\label{table:nao_blocks}
\begin{tabular}{|r|p{12cm}|}
    \hline                             
    \multicolumn{2}{|c|}{Nao action blocks}\\
    \hline
     {} & {} \\
                             Visual & \parbox[c]{1em}{\includegraphics[width=6cm]{../thesis/assets/blocks/nao_move.png}} \\
                             Name   & Move straight action \\
                             Description & The robot moves straight for the specified distance. \\
                             Parameters & 1. Distance in metres \\ 
                             {}         & 2. Name/Id of robot \\
    \hline
    {} & {} \\
                             Visual & \parbox[c]{1em}{\includegraphics[width=5cm]{../thesis/assets/blocks/nao_turn.png}} \\
                             Name   & Turn action \\
                             Description & The robot turn towards the direction specified \\
                             Parameters & 1. Direction: Left, Right, Around \\
                             {}         & 2. Name/Id of robot \\
    \hline
    {} & {} \\
                             Visual & \parbox[c]{1em}{\includegraphics[width=10cm]{../thesis/assets/blocks/nao_approach.png}} \\
                             Name   & Approach action \\
                             Description & The robot approaches towards the human who interacted with the robot through a gesture/voice command until the specified distance threshold is reached. \\
                             Parameters & 1. Distance threshold in metres \\
                             {}         & 2. Name/Id of robot\\

    \hline
    {} & {} \\
                             Visual & \parbox[c]{1em}{\includegraphics[width=5cm]{../thesis/assets/blocks/nao_behavior.png}} \\
                             Name   & Standard action \\
                             Description & The robot performs certain standard actions \\
                             Parameters & 1. Action name: Greet, Stand, Sit etc., \\
                             {}         & 2. Name/Id of robot \\

    \hline
    {} & {} \\
                             Visual & \parbox[c]{1em}{\includegraphics[width=6.5cm]{../thesis/assets/blocks/nao_say.png}} \\
                             Name   & Say expressively action \\
                             Description & The robot speaks the input text expressively (using hand and head motions)  \\
                             Parameters & 1. Text to speak \\
                             {}         & 2. Name/Id of robot \\

    \hline
    {} & {} \\
                             Visual & \parbox[c]{1em}{\includegraphics[width=8cm]{../thesis/assets/blocks/nao_arg_say.png}} \\
                             Name   & Say expressive with argument action \\
                             Description & A variable could be used in the text to speak  \\
                             Parameters & 1. Prefix/Suffix text with variable placeholder \\
                             {}         & 2. Name/Id of robot \\

    \hline
    {} & {} \\
                             Visual & \parbox[c]{1em}{\includegraphics[width=2cm]{../thesis/assets/blocks/nao_rest.png}} \\
                             Name   & Rest \\
                             Description & Put the robot in rest mode and switch off the motors (to avoid heating and conserve power)  \\
                             Parameters & 1. Name/Id of robot \\
     
    \hline
\end{tabular}
\end{table}
\subsection*{Behavior Designer} The Behavior designer surface could be used by the user to drag and drop the behavior blocks. The behavior designer is shown in Fig~\ref{fig:behavior_designer}.
The behavior designer is the main component of the user interface. The designer is composed of 
\begin{itemize}
\item Designer command panel : It contains command buttons like Open/Save/Clear/Run which makes it easy to perform operations like creating a new program, saving the program to the server, Clear the workspace and Running the active behavior program
\item Toolbox : It contains behavior, action and trigger blocks needed to realize the behavior program. It also contains the standard logical blocks that comes with Blockly editor. The toolbox components are categorized according to their functionality. The description of user interface blocks to build scenarios apart from the basic constructs of behavior program are shown in Tables~\ref{table:multi_blocks},\ref{table:nao_blocks}. These custom blocks are written in Javascript.
\item Designer surface: The designer surface is the playground where the user can create behavior program by dropping in blocks from the toolbox and build the program.
\end{itemize}
\subsection*{Visualization} The visualization could be used to see the interaction of the human and robot inside a virtual 3D environment.
% \begin{algorithm}
%  \KwData{simulation\_config}
%  \textbf{\emph{Init}}:\\
%  \quad INIT\_SIMULATION\_ENGINE(simulation\_config) \;
%  \quad INIT\_SUBSCRIBERS(simulation\_config) \;
%  \quad LOAD\_ENVIRONMENT(simulation\_config) \; 
%  \While{True}{
%   sensors = READ\_SENSOR\_VALUES() \; 
%   skeletons = READ\_SKELETON\_DATA() \; 
%   RENDER(robot,sensors)
%  }
%  \caption{Visualization algorithm}
%  \label{alg:visualize}
% \end{algorithm}
\subsection*{Monitor}
The monitor screen shows the current content of the application context in a tree view. This is mainly developed for debugging purpose.

\section{Summary}
This chapter discussed the software architecture of the Indriya platform. The specific implementation of a set of components of each layer in the software architecture are described which closes the design to execution loop of an HRI scenario. The Indriya platform built on latest software technologies also gives possibility to extend it easily because to its modular design. The decentralized approach makes it easy to integrate new sensors like those in smartphones, wearables etc., and new social robots as well. Corresponding visual blocks could be created which will be available to the end-user via the user interface and could be used to create HRI scenarios.
